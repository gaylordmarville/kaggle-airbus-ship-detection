{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting CUDA devices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting CUDA devices...\")\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import SaveBestModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = '/media/data-nvme/dev/datasets/airbus/'\n",
    "PATH = DATASET_ROOT\n",
    "TRAIN = DATASET_ROOT + 'train_v2/'\n",
    "TEST = DATASET_ROOT + 'test_v2/'\n",
    "SEGMENTATION = DATASET_ROOT + 'train_ship_segmentations_v2.csv'\n",
    "PRETRAINED = DATASET_ROOT + 'models/resnet34s256_kaggle-airbus-l0.053-a0.981.h5'\n",
    "DETECTION_TEST_PRED = DATASET_ROOT + 'models/ship_detection.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = ['6384c3e78.jpg'] #corrupted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = 12   #number of workers for data loader\n",
    "arch = resnet34 #specify target architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = [f for f in os.listdir(TRAIN)]\n",
    "test_names = [f for f in os.listdir(TEST)]\n",
    "for el in exclude_list:\n",
    "    if(el in train_names): train_names.remove(el)\n",
    "    if(el in test_names): test_names.remove(el)\n",
    "#5% of data in the validation set is sufficient for model evaluation\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.05, random_state=42)\n",
    "segmentation_df = pd.read_csv(os.path.join(PATH, SEGMENTATION)).set_index('ImageId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_empty(names):\n",
    "    return [name for name in names \n",
    "            if(type(segmentation_df.loc[name]['EncodedPixels']) != float)]\n",
    "\n",
    "tr_n = cut_empty(tr_n)\n",
    "val_n = cut_empty(val_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img_id, df):\n",
    "    shape = (768,768)\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    masks = df.loc[img_id]['EncodedPixels']\n",
    "    if(type(masks) == float): return img.reshape(shape)\n",
    "    if(type(masks) == str): masks = [masks]\n",
    "    for mask in masks:\n",
    "        s = mask.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, path, transform):\n",
    "        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        img = open_image(os.path.join(self.path, self.fnames[i]))\n",
    "        if self.sz == 768: return img \n",
    "        else: return cv2.resize(img, (self.sz, self.sz))\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        mask = np.zeros((768,768), dtype=np.uint8) if (self.path == TEST) \\\n",
    "            else get_mask(self.fnames[i], self.segmentation_df)\n",
    "        img = Image.fromarray(mask).resize((self.sz, self.sz)).convert('RGB')\n",
    "        return np.array(img).astype(np.float32)\n",
    "    \n",
    "    def get_c(self): return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLighting(Transform):\n",
    "    def __init__(self, b, c, tfm_y=TfmType.NO):\n",
    "        super().__init__(tfm_y)\n",
    "        self.b,self.c = b,c\n",
    "\n",
    "    def set_state(self):\n",
    "        self.store.b_rand = rand0(self.b)\n",
    "        self.store.c_rand = rand0(self.c)\n",
    "\n",
    "    def do_transform(self, x, is_y):\n",
    "        if is_y and self.tfm_y != TfmType.PIXEL: return x  #add this line to fix the bug\n",
    "        b = self.store.b_rand\n",
    "        c = self.store.c_rand\n",
    "        c = -1/(c-1) if c<0 else c+1\n",
    "        x = lighting(x, b, c)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    #data augmentation\n",
    "    aug_tfms = [RandomRotate(20, tfm_y=TfmType.CLASS),\n",
    "                RandomDihedral(tfm_y=TfmType.CLASS),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.CLASS)]\n",
    "    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS, \n",
    "                aug_tfms=aug_tfms)\n",
    "    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n",
    "    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), \n",
    "                (val_n,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "#     md.is_multi = False\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut,lr_cut = model_meta[arch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base():                   #load ResNet34 model\n",
    "    layers = cut_model(arch(True), cut)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def load_pretrained(model, path): #load a model pretrained on ship/no-ship classification\n",
    "    weights = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p))\n",
    "\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()\n",
    "    \n",
    "class Unet34(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        self.rn = rn\n",
    "        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x[:,0]\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()\n",
    "            \n",
    "class UnetModel():\n",
    "    def __init__(self,model,name='Unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2.0 * (pred*targs).sum() / ((pred+targs).sum() + 1.0)\n",
    "\n",
    "def IoU(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    intersection = (pred*targs).sum()\n",
    "    return intersection / ((pred+targs).sum() - intersection + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_when_acc(self, metrics):\n",
    "#     loss, acc = metrics[0], metrics[1]\n",
    "#     print(loss, acc)\n",
    "    filename = \"{}-{}-{}\".format(self.name, self.epoch, random.randint(1,1000))\n",
    "    print(filename)\n",
    "    self.model.save(f'{filename}')\n",
    "#     if self.best_acc == None or acc > self.best_acc:\n",
    "#         self.best_acc = acc\n",
    "#         self.best_loss = loss\n",
    "#         self.model.save(f'{filename}')\n",
    "#     elif acc == self.best_acc and  loss < self.best_loss:\n",
    "#         self.best_loss = loss\n",
    "#         self.model.save(f'{filename}')\n",
    "SaveBestModel.save_when_acc = save_when_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_base = load_pretrained(get_base(),PRETRAINED)\n",
    "m = to_gpu(Unet34(m_base))\n",
    "models = UnetModel(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sz = 384 #image size\n",
    "# bs = 28  #batch size\n",
    "\n",
    "# md = get_data(sz,bs)\n",
    "\n",
    "# learn = ConvLearner(md, models)\n",
    "# #learn.model = torch.nn.DataParallel(learn.model)\n",
    "# learn.opt_fn=optim.Adam\n",
    "# learn.crit = MixedLoss(10.0, 2.0)\n",
    "# learn.metrics=[accuracy_thresh(0.5),dice,IoU]\n",
    "# wd=1e-7\n",
    "# lr = 1e-2\n",
    "# lrs = np.array([lr/100,lr/10,lr])\n",
    "# learn.load('Unet34_s256_kaggle-airbus-l0.220-a0.999')\n",
    "# md = get_data(sz,bs)\n",
    "# learn.set_data(md)\n",
    "# learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "\n",
    "# learn.fit(lrs/5,2,wds=wd,cycle_len=2,use_clr=(10,8), best_save_name='Unet34_s' + str(sz) + \"_kaggle-airbus\")\n",
    "\n",
    "# sz = 768 #image size\n",
    "# bs = 6  #batch size\n",
    "# md = get_data(sz,bs)\n",
    "# learn.set_data(md)\n",
    "# learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "# learn.fit(lrs/10,2,wds=wd,cycle_len=1,use_clr=(10,8), best_save_name='Unet34_s' + str(sz) + \"_kaggle-airbus\")\n",
    "# print(\"---------- End of \", i, \"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SaveBestModel.save_when_acc(SaveBestModel, metrics=learn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a3be777b4e4162987a76f8c8d0f443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet34_s256i1_kaggle-airbus1-1-138                           \n",
      "epoch      trn_loss   val_loss   <lambda>   dice       IoU        \n",
      "    0      0.303294   0.279718   0.998221   0.815545   0.689778  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2797177008692306,\n",
       " 0.9982207231292443,\n",
       " 0.8155451332134592,\n",
       " 0.6897779966458375]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf53ec1afe74675a962bb403c2868a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet34_s256i1_kaggle-airbus2-1-429                           \n",
      "epoch      trn_loss   val_loss   <lambda>   dice       IoU        \n",
      "    0      0.267235   0.247474   0.998384   0.834922   0.717507  \n",
      "Unet34_s256i1_kaggle-airbus2-3-635                           \n",
      "    1      0.260556   0.245859   0.998399   0.836264   0.719571  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24585858984487113,\n",
       " 0.998399386013722,\n",
       " 0.8362640496102367,\n",
       " 0.7195707874685912]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72350a70a4c48049f5cf7ed318951be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet34_s256i1_kaggle-airbus3-1-525                           \n",
      "epoch      trn_loss   val_loss   <lambda>   dice       IoU        \n",
      "    0      0.242365   0.235443   0.998452   0.842381   0.728554  \n",
      "Unet34_s256i1_kaggle-airbus3-3-484                           \n",
      "    1      0.231534   0.22871    0.998494   0.846581   0.734801  \n",
      "Unet34_s256i1_kaggle-airbus3-5-824                           \n",
      "    2      0.244881   0.231935   0.998484   0.845139   0.732619  \n",
      "Unet34_s256i1_kaggle-airbus3-7-685                           \n",
      "    3      0.22333    0.21986    0.998537   0.851578   0.742318  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2198604280869313,\n",
       " 0.9985367511424913,\n",
       " 0.8515780853475969,\n",
       " 0.7423179190392415]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b745d3f8d96741ef88745ad741ce8cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1439/1442 [21:04<00:02,  1.14it/s, loss=0.225]"
     ]
    }
   ],
   "source": [
    "training_loop = [\n",
    "    [1, 2, (20,8), 256, 64],\n",
    "    [1, 2, (20,8), 256, 64],\n",
    "    [1, 2, (20,8), 512, 32]\n",
    "]\n",
    "i = 0\n",
    "for epochs, cycle_len, use_clr, sz, bs in training_loop:\n",
    "    i+=1\n",
    "    sz = 256 #image size\n",
    "    bs = 64  #batch size\n",
    "    md = get_data(sz,bs)\n",
    "    learn = ConvLearner(md, models)\n",
    "    learn.opt_fn=optim.Adam\n",
    "    learn.crit = MixedLoss(10.0, 2.0)\n",
    "    learn.metrics=[accuracy_thresh(0.5),dice,IoU]\n",
    "    wd=1e-7\n",
    "    lr = 1e-2\n",
    "    learn.freeze_to(1)\n",
    "    #learn.model = torch.nn.DataParallel(learn.model)\n",
    "    print(\"SZ:\", sz, \"BS:\", bs)\n",
    "    learn.fit(lr,1,wds=wd,cycle_len=1,use_clr=(5,8), best_save_name='Unet34_s' + str(sz) + 'i' + str(i) + \"_kaggle-airbus1\")\n",
    "    lrs = np.array([lr/100,lr/10,lr])\n",
    "    learn.unfreeze() #unfreeze the encoder\n",
    "    learn.bn_freeze(True)\n",
    "    print(\"SZ:\", sz, \"BS:\", bs)\n",
    "    learn.fit(lrs,2,wds=wd,cycle_len=1,use_clr=(20,8), best_save_name='Unet34_s' + str(sz) + 'i' + str(i)  + \"_kaggle-airbus2\")\n",
    "    print(\"SZ:\", sz, \"BS:\", bs)\n",
    "    learn.fit(lrs/3,2,wds=wd,cycle_len=2,use_clr=(20,8), best_save_name='Unet34_s' + str(sz) + 'i' + str(i)  + \"_kaggle-airbus3\")\n",
    "\n",
    "    sz = 384 #image size\n",
    "    bs = 28  #batch size\n",
    "    md = get_data(sz,bs)\n",
    "    learn.set_data(md)\n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "    print(\"SZ:\", sz, \"BS:\", bs)\n",
    "    learn.fit(lrs/5,2,wds=wd,cycle_len=2,use_clr=(10,8), best_save_name='Unet34_s' + str(sz)  + 'i' + str(i) + \"_kaggle-airbus4\")\n",
    "\n",
    "    sz = 768 #image size\n",
    "    bs = 6  #batch size\n",
    "    md = get_data(sz,bs)\n",
    "    learn.set_data(md)\n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "    print(\"SZ:\", sz, \"BS:\", bs)\n",
    "    learn.fit(lrs/10,2,wds=wd,cycle_len=1,use_clr=(10,8), best_save_name='Unet34_s' + str(sz)  + 'i' + str(i) + \"_kaggle-airbus5\")\n",
    "    print(\"---------- End of \", i, \"------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('Unet34_s256_kaggle-airbus-l0.222-a0.999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Show_images(x,yp,yt):\n",
    "    columns = 3\n",
    "    rows = min(bs,8)\n",
    "    fig=plt.figure(figsize=(columns*4, rows*4))\n",
    "    for i in range(rows):\n",
    "        fig.add_subplot(rows, columns, 3*i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(x[i])\n",
    "        fig.add_subplot(rows, columns, 3*i+2)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(yp[i])\n",
    "        fig.add_subplot(rows, columns, 3*i+3)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(yt[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.eval();\n",
    "x,y = next(iter(md.val_dl))\n",
    "yp = to_np(F.sigmoid(learn.model(V(x))).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show_images(np.asarray(md.val_ds.denorm(x)), yp, y.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
